okta = {
  # Required for any Okta related installation
  org_name = "<your-okta-org-name>"
  base_url = "okta.com"
  tfauth = {
    client_id = "<your-okta-client-id>"
    # Minimum scopes: apps, app grants (scope consent), roles, and policies (provider sets OAuth app authentication/access policy; see README).
    scopes = ["okta.apps.manage", "okta.apps.read", "okta.appGrants.manage", "okta.roles.manage", "okta.roles.read", "okta.policies.read", "okta.policies.manage"]
    # Set the OKTA_API_PRIVATE_KEY environment variable to the private key value starting with -----BEGIN PRIVATE KEY-----
    # OKTA_API_PRIVATE_KEY must be in PEM format when it is output from the Okta UI
    # Alternatively, use the jwk-to-pem.py utility to convert a JWK private key to a PEM private key
    private_key_id = "<your-okta-private-key-id>"
  }
  # Required for Okta login installation
  native_app = {
    app_name          = "<your-okta-native-app-name>"
    app_redirect_uris = ["https://p0.app/oidc/auth/_redirect"]
  }
  # Required for Okta group listing installation
  api_integration_app = {
    app_name = "<your-okta-api-integration-app-name>"
  }
}

p0 = {
  org_id = "<your-p0-org-id>"
}

aws = {
  # Optional grouping tag for SSH; see https://docs.p0.dev/integrations/resource-integrations/ssh (e.g. p0 request ssh group --name <value>)
  group_key = "environment"
}

kubernetes = {
  cluster = {
    # This is the name of the cluster as it will appear in P0; conventionally same as EKS cluster name
    id = "sandbox-eks-cluster"
    
    # In the AWS EKS console, the field is called Cluster ARN
    arn = "arn:aws:eks:us-west-2:ACCOUNTNUMBER:cluster/CLUSTERNAME"
    
    # In the AWS EKS console, the field is called Server API Endpoint
    endpoint = "https://<RANDOM>.us-west-2.eks.amazonaws.com"
    
    # In the AWS EKS console, the field is called Certificate Authority
    cert_authority = "ARATHERLONGCERTIFICATESTRING"
    
    # The region in which the EKS cluster exists
    region = "us-west-2"
  }
}

# Used by the SSH integration: SSM VPC endpoints are created in each listed VPC so EC2 instances there can be managed via P0/Systems Manager.
#
# To find VPC IDs:
#   - AWS Console: VPC â†’ Your VPCs (per region).
#   - CLI (table with VpcId and Name tag):
#       aws ec2 describe-vpcs --region us-west-1 
#       aws ec2 describe-vpcs --region us-west-2
#     For another region, replace <region> in: aws ec2 describe-vpcs --region <region> --query 'Vpcs[].[VpcId,Tags[?Key==`Name`].Value|[0]]' --output table
regional_aws = {
  us-west-1 = {
    enabled_vpcs                    = ["<your-us-west-1-vpc-id>"]
    is_resource_explorer_aggregator = false
  }
  us-west-2 = {
    enabled_vpcs                    = ["<your-us-west-2-vpc-id>"]
    is_resource_explorer_aggregator = true
  }
}

datadog = {
  # Required for Datadog audit logs integration
  intake_url        = "https://http-intake.logs.us5.datadoghq.com"
  api_key_cleartext = "<your-datadog-api-key>"
}
